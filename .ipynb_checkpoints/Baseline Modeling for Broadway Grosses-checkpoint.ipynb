{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this capstone project I am going attempt to predict when a Broadway show should close.  The idea behind this development is that an interface would be developed for producers that would allow them to enter their numbers from the previous week and determine if they are in a \"red zone\", therefore closing.  \n",
    "\n",
    "I have divided the project up into three parts:\n",
    "\n",
    "**Baseline Modeling**\n",
    "In this section is an indepth Exploratory Data Analysis of Broadway weekly grosses, types of shows, and other important features that may influence whether a show will close or not.  SVM and Random Forest are used as baseline modeling techniques while addressing things like class imabalance.  \n",
    "\n",
    "**ANN for Broadway Grosses**\n",
    "Using the Keras API an Artificial Neural Network is constructed to use sophisticated algorithms to determine if a Broadway show should close or not.\n",
    "\n",
    "**Wicked Time Series Analysis**\n",
    "The core to this data is the passage of time and how much money the show is making.  It is essential to take an in depth look at time series, how it can influence our predictions, and its components like seasonality.  It is concluded with a prediction of how much money the Broadway show Wicked lost the first year of COVID-19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.323Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data was orginally parsed from the Broadway League website and can be found [here](https://www.broadwayleague.com/research/grosses-broadway-nyc/#weekly_grosses)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Broadway League is an internationally recognized organization that maintains the standards and efficacy of Broadway shows.  They are not connected with the production companies and they do not record the grosses.  The grosses are recorded by the box office that remains a neutral 3rd party.  \n",
    "The original data came with 12 rows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Feature|Description|\n",
    "|:----:|:----:|\n",
    "|date|Past 5 years of all Broadway data.|\n",
    "|show|Name of the show.|\n",
    "|type|Distinguished by Play, Musical, or Special.|\n",
    "|theatre|Which physical theatre the show takes place at.|\n",
    "|previews|How many of the performances were previews.|\n",
    "|performances|How many performances occured not including previews.|\n",
    "|grosses|Total revenue generated before running costs.|\n",
    "|prev week grosses|The revenue generated from the previous week.|\n",
    "|GG%GP|This is the percentage of the revenue generated out of the possible grossings. Number is defined by every seat in the house being sold at full price.|\n",
    "|attendance|How many people attended the Performance.|\n",
    "|prev week attendance|How many people attended last week.|\n",
    "|%capacity|How much of the theatre was full.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feture Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition I added four rows as additional features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Feature|Description|\n",
    "|:---:|:---:|\n",
    "|Close Month| This demarcates the month of closing.|\n",
    "|Genre|Includes: Mystery, Comedy, Drama, Alternative, Jukebox, Tragedy|\n",
    "|Tony Noms|The number of Tony nominations the show recieved.|\n",
    "|Tony Awards|The number of Tony awards the show won.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Features were all originally collected and stored in a large Excel file.  \n",
    "Creating a closing month feature was the vital column that my argument leans on.  The idea is that we get the computer to pick up on the fact that the show is not doing well and therefore is closing.  By marking the last month of a shows life we can try to run sophisticated algorithms that attempt to detect when that will be based on other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.332Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load data\n",
    "df = pd.read_excel('Broadway_Grosses.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.336Z"
    }
   },
   "outputs": [],
   "source": [
    "##Initial look at the data and it read in nicely.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.338Z"
    }
   },
   "outputs": [],
   "source": [
    "#Looking to makes sure that shows are only categorized as musical, play, or special.\n",
    "df['type'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.341Z"
    }
   },
   "outputs": [],
   "source": [
    "df['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.344Z"
    }
   },
   "outputs": [],
   "source": [
    "#Noting that we have over 8,000 rows but only working with 233 shows.\n",
    "print('----------------------------------------------------------')\n",
    "print('The number of Broadway Shows in this dataset is:',df['show'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.347Z"
    }
   },
   "outputs": [],
   "source": [
    "df['show'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.350Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is an extremely clean and tight data set since it was custom created.  \n",
    "# The 213 missing data points account for the first week of a show since there would be no previous records for a new show.\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.353Z"
    }
   },
   "outputs": [],
   "source": [
    "# Changing the show names from all caps to standard format.\n",
    "df['show'] = df['show'].str.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.355Z"
    }
   },
   "outputs": [],
   "source": [
    "#getting rid of the few null values.\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.359Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.362Z"
    }
   },
   "outputs": [],
   "source": [
    "#For whatever reason prev_week_gross is an object and we need it as an integer or float for analytic purposes.\n",
    "df['prev_week_gross'] = df['prev_week_gross'].values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.364Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.369Z"
    }
   },
   "outputs": [],
   "source": [
    "#Taking a general look at numbers.  This is pretty normally distributed data with some outliers.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.372Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.375Z"
    }
   },
   "outputs": [],
   "source": [
    "#setting the date as the index for ease of use, note that object type was imported as a datetime64.\n",
    "df = df.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.379Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.383Z"
    }
   },
   "outputs": [],
   "source": [
    "#A histogram of all quantitative features is usually my first step.  \n",
    "#  Although this is quick and rough it shows us that we have very pretty evenly distributed data\n",
    "df.hist(figsize=(15,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to take a deeper look into 3 specific shows: Hamilton, Anastasia, and Beetlejuice.\n",
    "Items to note about each show as follows:\n",
    "\n",
    "**Hamilton**- Definitely what we consider an anomolie.  The success of a show like Hamilton is extremely rare and we see by the visualizations of their data that it has abnormal habits.  Not a great show data wise to reference.\n",
    "\n",
    "**Anastasia**-  This show is your average broadway show.  It ran for over a year and opened to great numbers that were sometimes over 1 million a week threshold. However we can see its decline very clearly from the graphs.\n",
    "\n",
    "**Beetlejuice**-  The newcomer underdog.  The show did not open to raving success but quickly became a cult phenomenon. We can see its giant uptick in sales visually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamilton Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.390Z"
    }
   },
   "outputs": [],
   "source": [
    "#creating three data frames each for the respective show.\n",
    "df_hamilton = df[df.show.str.contains('Hamilton')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.394Z"
    }
   },
   "outputs": [],
   "source": [
    "df_hamilton.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beetlejuice Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.397Z"
    }
   },
   "outputs": [],
   "source": [
    "df_beetlejuice = df[df.show.str.contains('Beetlejuice')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.401Z"
    }
   },
   "outputs": [],
   "source": [
    "df_beetlejuice.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anastasia Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.405Z"
    }
   },
   "outputs": [],
   "source": [
    "df_anastasia = df[df.show.str.contains('Anastasia')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.408Z"
    }
   },
   "outputs": [],
   "source": [
    "df_anastasia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.412Z"
    }
   },
   "outputs": [],
   "source": [
    "#importing plotly\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### % Capacity Filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HAMILTON**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sort of time plot for a Broadway show is definitely unprecedented.  The graph looks extremely random, but the show is in such high demand that if you look at the %Capacity on the y axis, the capacity only wavers by a 100th of a percent, therefore giving this graph an unusual look.  All capacity is above 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.417Z"
    }
   },
   "outputs": [],
   "source": [
    "px.line(df_hamilton, x=df_hamilton.index, y='%cap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEETLEJUICE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there is a lot of seasonality in this graph, you can see an overall upward trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.422Z"
    }
   },
   "outputs": [],
   "source": [
    "px.line(df_beetlejuice, x=df_beetlejuice.index, y='%cap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANASTASIA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same idea, although there are major seasonality and marketing spikes, there is a clear negative relationship with the amount of people coming to this show over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.426Z"
    }
   },
   "outputs": [],
   "source": [
    "px.line(df_anastasia, x=df_anastasia.index, y='%cap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly Grosses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HAMILTON**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to assume that merchandise is a large part of these large numbers.  Although they are able to sell the tickets at a steep price for this show a 4 million dollar week on Broadway is unheard of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.431Z"
    }
   },
   "outputs": [],
   "source": [
    "px.line(df_hamilton, x=df_hamilton.index, y='grosses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEETLEJUICE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very strong positive linear trend for these grosses.  We can see visually it would be a good idea to keep this show open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.435Z"
    }
   },
   "outputs": [],
   "source": [
    "px.line(df_beetlejuice, x=df_beetlejuice.index, y='grosses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANASTASIA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again a downward trend, negative linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.439Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "from matplotlib import pyplot\n",
    "\n",
    "fig, ax = pyplot.subplots(figsize=(20,8))\n",
    "\n",
    "ax = sns.lineplot(data=df_anastasia, x=df_anastasia.index, y='grosses')\n",
    "ax.axvspan(*mdates.datestr2num(['01/01/2019', '03/31/2019']), color='red', alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"Date\", fontsize=20)\n",
    "plt.ylabel(\"Anastasia Grosses\", fontsize=20)\n",
    "plt.title(\"It's good to know when it's time to close.\", fontsize=26)\n",
    "plt.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph is a crucial visualization of how our 'close_date' feature works.  It marks the last month of performances.  The idea is tha the model will evaluate all of the other features happening during this band of time and begin to form pattern recognition in order to flag it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the average grossings by type. I believe that the average grossings for Special performances is so high due to the fact that they have predetermined closing dates making them higher in demand.  We are eventually going to drop all special performances for this reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.444Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.barplot(data=df, x='type', y='grosses', palette = 'rocket_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.446Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = pyplot.subplots(figsize=(20,8))\n",
    "\n",
    "g = sns.barplot(data=df, x='genre', y='grosses', palette= 'crest')\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=45, ha='right', fontsize=14)\n",
    "\n",
    "plt.xlabel(\"Genre\", fontsize=20)\n",
    "plt.ylabel(\"Average Weekly Gross\", fontsize=20)\n",
    "plt.title(\"Grosses by Genre\", fontsize=32)\n",
    "plt.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Musicals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.449Z"
    }
   },
   "outputs": [],
   "source": [
    "#creating a data frame from all musicals\n",
    "df_musical = df[df.type.str.contains('Musical')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.451Z"
    }
   },
   "outputs": [],
   "source": [
    "df_musical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.454Z"
    }
   },
   "outputs": [],
   "source": [
    "#The number of musicals in this data set\n",
    "df_musical.show.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.461Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = pyplot.subplots(figsize=(30,14))\n",
    "\n",
    "#set to g and make adjustments.\n",
    "g = sns.boxplot(data=df_musical, x='show', y='grosses', palette='crest')\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.463Z"
    }
   },
   "outputs": [],
   "source": [
    "df_noms = df_musical[df_musical['tony_noms']>=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.465Z"
    }
   },
   "outputs": [],
   "source": [
    "df_noms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.468Z"
    }
   },
   "outputs": [],
   "source": [
    "df_noms.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.471Z"
    }
   },
   "outputs": [],
   "source": [
    "#creating a df for visualization that sorts tony nominations from most to least and filters out no nominations.\n",
    "df_noms_final = df_noms.sort_values('tony_noms', ascending=False)\n",
    "df_noms_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.473Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = pyplot.subplots(figsize=(30,8))\n",
    "\n",
    "ax = sns.barplot(data=df_noms_final, x='show', y='tony_noms', palette='inferno_r')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.xlabel(\"Show\", fontsize=20)\n",
    "plt.ylabel(\"Number of Tony Nominations\", fontsize=20)\n",
    "plt.title(\"Number of Tony Noms per Show\", fontsize=32)\n",
    "plt.tight_layout\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important to note what shows won a lot of tony awards versus what stayed open and made money.  Many times what the artistic community finds entrinsically valuable is not the same as what the public places value on.\n",
    "Shuffle along winning 10 Tony nominations and closing after a few months at a low house capacity.\n",
    "Aladdin only won 5 and remains one of the top grossing shows on Broadway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.478Z"
    }
   },
   "outputs": [],
   "source": [
    "#creating a data frame for all plays.\n",
    "df_play = df[df.type.str.contains('Play')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.484Z"
    }
   },
   "outputs": [],
   "source": [
    "df_play.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.487Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = pyplot.subplots(figsize=(30,14))\n",
    "\n",
    "g = sns.boxplot(data=df_play, x='show', y='grosses', palette='crest')\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.489Z"
    }
   },
   "outputs": [],
   "source": [
    "#Looking at the amount the house was filled v how much it made.  \n",
    "#No surprise strong linear trend.\n",
    "fig, ax = pyplot.subplots(figsize=(20,14))\n",
    "sns.lineplot(data=df, x='%cap', y='grosses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomolie Detection through EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.493Z"
    }
   },
   "outputs": [],
   "source": [
    "#We need to get rid of all special performances as they will influence our data negatively.\n",
    "df = df[~df.type.str.contains(\"Special\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.497Z"
    }
   },
   "outputs": [],
   "source": [
    "#Checking to make sure that we did not loose too many data points.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hote Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.500Z"
    }
   },
   "outputs": [],
   "source": [
    "#get dummies\n",
    "df = pd.get_dummies(df, columns = ['show', 'type', 'theatre', 'genre'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.502Z"
    }
   },
   "outputs": [],
   "source": [
    "#These numbers will be important later for us to know how many columns that created.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATE VARIABLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.505Z"
    }
   },
   "outputs": [],
   "source": [
    "#set X equal to our closing data and the rest become features.\n",
    "X = df.drop('close_month', axis = 1)\n",
    "y = df['close_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.508Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression methods were used for this situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A support vector machine takes data points and outputs the hyperplane (which in two dimensions it's simply a line) that best separates the features. This line is the decision boundary: anything that falls to one side of it we will classify as say, 'blue', and anything that falls to the other as 'red'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way Random Forests reduce variance is by training on different samples of the data. A second way is by using a random subset of features. This means if we have 30 features, random forests will only use a certain number of those features in each model, say five. Unfortunately, we have omitted 25 features that could be useful. But as stated, a random forest is a collection of decision trees. Thus, in each tree we can utilize five random features. If we use many trees in our forest, eventually many or all of our features will have been included. This inclusion of many features will help limit our error due to bias and error due to variance. If features werenâ€™t chosen randomly, base trees in our forest could become highly correlated. This is because a few features could be particularly predictive and thus, the same features would be chosen in many of the base trees. If many of these trees included the same features we would not be combating error due to variance. With that said, random forests are a strong modeling technique and much more robust than a single decision tree. They aggregate many decision trees to limit overfitting as well as error due to bias and therefore yield useful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.516Z"
    }
   },
   "outputs": [],
   "source": [
    "#import model\n",
    "from sklearn.svm import SVC\n",
    "svcls = SVC(kernel='rbf', C=1, gamma = 2**-5)\n",
    "# C(COST) is a hypermeter set before training model and used to control error\n",
    "# Gamma also a hypermeter which is set before the training model and used to give curvature weight of the decision boundary.\n",
    "svc = svcls.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.519Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_svc = svcls.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.521Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.524Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.526Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred_svc)\n",
    "print('Confusion Matrix:\\n', cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.529Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.532Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(svc, X_test, y_test, cmap=plt.cm.Blues, display_labels = ['Do not Close', 'Close'])\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the confusion matrix above is terrible.  I am unsure why the machine wants everything to be do not close but it is not ideal.  My original thought was because of the Class Imbalance in the closing month column.\n",
    "Class imbalance is handled below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason this is such an issue is that many classification learning algorithms have low predictive accuracy for the infrequent class.  We can see that above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.537Z"
    }
   },
   "outputs": [],
   "source": [
    "count_classes = pd.value_counts(df['close_month'], sort = True)\n",
    "count_classes.plot(kind = 'bar', rot=0)\n",
    "\n",
    "plt.title(\"Open and Closing Class Distribution\")\n",
    "plt.xticks((0,1), ('Open', 'Closing'))\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE stands for Synthetic Minority Oversampling Technique. This is a statistical technique for increasing the number of cases in your dataset in a balanced way.  SMOTE takes the entire dataset as an input, but it increases the percentage of only the minority cases.  Similar to boostrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.541Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "X_sm, y_sm = sm.fit_resample(X, y)\n",
    "\n",
    "\n",
    "##finding shortcuts for printing clean data results\n",
    "print(f'''Shape of X before SMOTE: {X.shape}\n",
    "Shape of X after SMOTE: {X_sm.shape}''')\n",
    "\n",
    "print('\\nBalance of positive and negative classes (Percentage):')\n",
    "y_sm.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.546Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sm, y_sm, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Rerun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.549Z"
    }
   },
   "outputs": [],
   "source": [
    "svc = svcls.fit(X_train, y_train)\n",
    "y_pred_svc = svcls.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.551Z"
    }
   },
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_test, y_pred_svc)\n",
    "print('Confusion Matrix:\\n', cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.554Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(svc, X_test, y_test, cmap=plt.cm.Blues, display_labels = ['Do Not Close', 'Close'])\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.557Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results still did not run well for this, but lets move on to Random Forrest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.560Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#max depth:\n",
    "#max leaf nodes:\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=32,  \n",
    "                             max_leaf_nodes=40)\n",
    "rf_clf = clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred_rf = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.563Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.566Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(rf_clf, X_test, y_test,  cmap=plt.cm.Blues, display_labels = ['Do Not Close', 'Close'] )\n",
    "plt.title('Confusion Matrix for Random Forest')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above classification report and confusion matrix are actually very nice looking. After adjusting the hyperparameters a few times I finally settled on what I have now which gave me an accuracy of 90% without being too overfit.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.569Z"
    }
   },
   "outputs": [],
   "source": [
    "## Initial overfitting in this plot\n",
    "from sklearn import tree\n",
    "plt.figure(figsize = (15,10))\n",
    "tree.plot_tree(rf_clf.estimators_[2], rounded = True, \n",
    "               filled = True, \n",
    "               class_names = ['Close', 'Do Note Close'], \n",
    "               feature_names = X.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A note on the overfitting**\n",
    "Finding the balance on over and underfitting on this model was extremely difficult.  There are still sometimes where the above chart looks very overfit.  \n",
    "My guess for all of the overfitting is that because long running shows keep aquiring new data without closing.  I am sure this is confusing to the model, still working on how to solve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is great that we can predict when a broadway show is going to close with such high accuracy, we also have to wonder what the main things were that influenced the computers decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.573Z"
    }
   },
   "outputs": [],
   "source": [
    "important_features_dict = {}\n",
    "for idx, val in enumerate(clf.feature_importances_):\n",
    "    important_features_dict[idx] = val\n",
    "\n",
    "important_features_list = sorted(important_features_dict,\n",
    "                                 key=important_features_dict.get,\n",
    "                                 reverse=True)\n",
    "\n",
    "print('9 most important features:', important_features_list[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.576Z"
    }
   },
   "outputs": [],
   "source": [
    "data = important_features_dict\n",
    "col = X.columns.tolist()\n",
    "\n",
    "df_fi = pd.DataFrame.from_dict(data, orient='index',\n",
    "                               columns=['feat_im'])\n",
    "df_fi['feature'] = col\n",
    "\n",
    "df_fi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.578Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sorted = df_fi.sort_values('feat_im', ascending=False)\n",
    "df_final = df_sorted.iloc[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.581Z"
    }
   },
   "outputs": [],
   "source": [
    "df_final.head(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ðŸ›Ž **IMPORTANT** \n",
    "Please note that that these labels must be changed according to what the result of the previous cell states.  Features are subject to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.584Z"
    }
   },
   "outputs": [],
   "source": [
    "# These are the original labels in order from the first time I ran this feature importance function.  \n",
    "# After running this multiple times, although the features shift around slightly this is a pretty good summation of the results.\n",
    "labels = ['Previous Weeks Attendance', '# Tony Nominations', 'Previous Weeks Gross', 'Current Gross', 'GG%GP',\n",
    "          '# Tony Awards', 'Jukebox Musical Genre', 'Current Attendance', 'Comedy Genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-09T11:47:53.586Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = pyplot.subplots(figsize=(20,8))\n",
    "\n",
    "g = sns.barplot(data=df_final, x='feature', y='feat_im', palette= 'crest')\n",
    "g.set_xticklabels(labels, rotation=45, ha='right', fontsize=14)\n",
    "\n",
    "plt.xlabel(\"Feature\", fontsize=20)\n",
    "plt.ylabel(\"Level of Importance\", fontsize=20)\n",
    "plt.title(\"Feature Importance of Closing a Broadway Show\", fontsize=32)\n",
    "plt.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the original graph above, it makes sense.  Its actually ideal that the data from the previous week is two of the first three most important features.  This means that the computer picked up on the fact that numbers were declining to a certain level and so it should classify it as time to close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.We can accurately predict when a show is in the 'red zone'. However, we do need to spend more time augmenting the data and acquiring more features.\n",
    "\n",
    "2.Random Forrest Model proved to be the most effective model.\n",
    "\n",
    "3.When looking at what features a Producer should have thier eye on.  We can conclude that they should be concerned with the over all decline of the shows numbers, i.e. always comparing the numbers from last week to the current week. And choosing a Jukebox musical or Comedy will have a large influence on how a show will do.  This makes sense when we look at the original EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "579px",
    "left": "40px",
    "top": "110px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
